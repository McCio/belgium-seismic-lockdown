\documentclass[12pt]{article}

% BY MINE REPORTS
\usepackage{fancyhdr}
\usepackage{lastpage} % last page ref
\usepackage{array}
\usepackage[table]{xcolor}
\usepackage{scrextend}
\usepackage{enumitem}
\usepackage{caption} % for image/tables/... captions
\usepackage{fancyvrb} % for centered verbatim
%\usepackage{hyperref} % for links in refs
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[round, sort, numbers]{natbib}
\bibliographystyle{unsrtnat}
% /BY MINE REPORTS

% BY VARIN
\usepackage[sc]{mathpazo}
\usepackage{amsthm,amsmath,amssymb}
\usepackage{titlesec}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\geometry{a4paper,verbose,tmargin=2.5cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\setcounter{secnumdepth}{2}
\setcounter{tocdepth}{2}
\usepackage{url}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=2,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 pdfstartview={XYZ null null 1}}
\usepackage{breakurl}
% /BY VARIN

% BY MINE REPORTS
\renewcommand{\labelenumii}{\theenumi\theenumii.}
\renewcommand{\theenumii}{.\arabic{enumii}}
\renewcommand{\labelenumiii}{\theenumi\theenumii\theenumiii.}
\renewcommand{\theenumiii}{.\arabic{enumiii}}

\newtheorem*{theorem}{Theorem}
\newtheorem*{proposition}{Proposition}

\pagestyle{fancy}
\fancyhf{}
% /BY MINE REPORTS

% FOR KNITR CHILD
%\usepackage[]{graphicx} %% used before
\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
	\ifdim\Gin@nat@width>\linewidth
	\linewidth
	\else
	\Gin@nat@width
	\fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
	\def\at@end@of@kframe{}%
	\ifinner\ifhmode%
	\def\at@end@of@kframe{\end{minipage}}%
\begin{minipage}{\columnwidth}%
	\fi\fi%
	\def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
		\colorbox{shadecolor}{##1}\hskip-\fboxsep
		% There is no \\@totalrightmargin, so:
		\hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
	\MakeFramed {\advance\hsize-\width
		\@totalleftmargin\z@ \linewidth\hsize
		\@setminipage}}%
{\par\unskip\endMakeFramed%
	\at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% / FOR KNITR CHILD

\renewcommand{\theequation}{\roman{equation}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\p}{p}
\DeclareMathOperator*{\cov}{cov}
\DeclareMathOperator*{\var}{var}
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\given}{\vert}
\newcommand*\mean[1]{\overline{#1}}
\DeclareMathOperator*{\simf}{sim}

\title{Seismic Time Series Analysis\\Statistics for Spatio-Temporal Data\\\textit{-- project draft --}}
\author{Marco Ciotola, 848222}
\date{\today}


\begin{document}
\pagenumbering{gobble}
\maketitle
\begin{figure}[t!]
	\begin{center}
		\includegraphics[width=100px]{ca_foscari_logo.png}
	\end{center}
\end{figure}
\vfill Prof. Isadora Antoniano Villalobos - CM0477
\hfill A.Y. 2019/2020
\newpage
\tableofcontents \clearpage
\fancyfoot[RO,LE]{Page \thepage \hspace{1pt} of \pageref{LastPage}}
\pagenumbering{arabic}
% /BY MINE REPORTS

% BY VARIN
%\setlength{\parindent}{0pt}
% /BY VARIN


\section{Introduction}

\subsubsection{Motivation and objective}
Thanks to the recent COVID-19 outbreak worldwide, we currently have data about many different phoenomena from periods with standard human activity, and from a period with reduced human activity.

Inspired by an article published during the initial phases of the Belgium-wide lockdown \cite{NatureCoronavirusSeismic}, we want to assess whether the lockdown period can be correlated with a visible change in background seismic activity.

It is important to notice that some industrial events, such as mine explosions, are already mapped by seismologic institutions since they interfere with seismic surveys \cite{OtherSeismicEvents}. Such events are sometimes comparable with natural seismic events, depending on the distance from the seismograph.

The correct estimation of a background noise, correlated or not with human activity, could help institutions to better isolate less prominent earthquakes and similar events from the seismic data. This could result in better understanding of the frequency of earthquakes and by studying their more-precise evolution over time.

\subsubsection{Data source and description}
The Royal Observatory of Belgium (ROB) gathers and publishes \cite{RoyalDataPolicy,Data20200402} data about vertical ground displacement from 9 stations through the country, linking them to seismic events whenever possible \cite{DataEvents20200402}. They also offer a data visualization tool for the same data \cite{DataVisualization20200402}.

The vertical ground displacement is represented by the minimum and maximum displacements of the ground in the vertical axis registered in a second, with $nm$ as unit of measure. This can easily be transformed to total ground displacement for each second by taking their difference. There are 86400 measurements per day, around 30 million per year.

Out of the 9 stations, the following 6 have interesting positions, related to possible correlations with human activity:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item \textbf{Uccle}, \textbf{Sart-Tilman}, and \textbf{Ében-Émael} are near a city (Brussels, Liege, and Maastricht).
	\item \textbf{Ostenda} This station is near the city of Ostenda, that borders with the sea.
	\item \textbf{Membach} and \textbf{Dourbes} are near a natural park.
\end{itemize}

Data collected from near a city and from near a natural park could be selected, in order to compare possible seasonality results and differences from before and after the lockdown. %Also data from \textbf{Ostenda} station could be interesting, given the possible sea influence.
Considering these factors, we selected the \textbf{Uccle} and \textbf{Membach} stations.

It is important to notice that the daily data in each file refers to the UTC time instead of the local Europe/Brussels time, an optimal choice to publish data in a machine-readable way. This causes an issue fitting a fixed seasonality, when it depends on the legal time rather than on the absolute time.

\subsection{Data cleanup and transformation}\label{sec:cleanup-transform}
\subsubsection{Data gathering and cleanup}
The data is provided in CSV format, one file per day, with only two columns related to the min-max pair \cite{Data20200402} and no column related to time. This causes two problems:
\begin{enumerate}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item For merging days, we need to add a column with the second-precise datetime
	\item In days with missing values there are placeholder values instead a missing row, that would make the following rows misinterpreted for measurements on the wrong second. The placeholder values are one of these two min-max pairs: $(0,0),\, (-1,1)$.
\end{enumerate}

Data from each station have different domains, as distinct equipment can register a specific amplitude of the seismic movements. In fact, we found some values\footnote{Values out of domain for each station are less than 10 out of $\approx50$ million.} outside the domains, and we consider them as missing values.

In particular, we find that out of $\approx50$ million seconds, from 27-10-2018 to 28-05-2020, only 0.67\% (Uccle) and 0.37\% (Membach) are missing values. We will see later how they are distributed for each station.

% Uccle, Bruxelles (UCCS station)
% Domain values: -350000 to 350000 nm
% Data from 2018-10-27 00:00:00 to 2020-05-29 00:12:07
% Removing 7 values out of domain, 28169 explicit NA
% 49779920 present values, 334345 missing points ( 99.33 % / 0.67 % )

% Membach (MEM station)
% Domain values: -7200 to 7200 nm
% Data from 2018-10-27 00:00:00 to 2020-05-29 00:09:55
% Removing 8 values out of domain, 7018 explicit NA
% 49929756 present values, 184421 missing points ( 99.63 % / 0.37 % )

\subsubsection{Data aggregation and initial transformation}
Since we are not interested in a second-precise analysis, and it would be computationally expensive to deal with the whole dataset, we aggregate data by hour. The aggregation results in around 14 thousand points, that can still be captured by interpretable models.

While performing this operation, we should consider the correct time-zone: any seasonal analysis dealing with UTC dates would be out of phase for the half-year of legal time, with respect to the half-year of solar time. With the help of the \texttt{lubridate} R package we converted times to local time-zones, and then we kept them as absolute values, because all the R functions we use work on the UTC conversion of datetimes by default. This transformation made it possible to have each hour in the timeseries to represent the local time in Belgium (\textit{Europe/Brussels}), without being influenced by the legal time.

The aggregation was performed considering the mean total ground displacement, for the following reasons:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item When an hour contains some missing seconds, these do not heavily affect the aggregation result, in contrast with summing the values
	\item Seismic events, such as quakes and mine explosions, normally last some minutes at most, and will affect the result only slightly, as opposed to the hourly maximum
	\item Usually, maximum value aggregation does not follow the Normal distribution, while some models that we are going to use assume the data follow a Normal distribution
\end{itemize}

\section{Analysis and decomposition}
\subsubsection{Missing values analysis}

% Uccle, Bruxelles (UCCS station)
% Domain values: -350000 to 350000 nm
% Data from 2018-10-27 00:00:00 to 2020-05-29 00:12:07
% Removing 7 values out of domain, 28169 explicit NA
% 49779920 present values, 334345 missing points ( 99.33 % / 0.67 % )
% 13831 present values, 90 missing points (99.35% / 0.65%)

% Membach (MEM station)
% Domain values: -7200 to 7200 nm
% Data from 2018-10-27 00:00:00 to 2020-05-29 00:09:55
% Removing 8 values out of domain, 7018 explicit NA
% 49929756 present values, 184421 missing points ( 99.63 % / 0.37 % )
% 13872 present values, 49 missing points (99.65% / 0.35%)

After aggregating by hour, the percentages of missing values remain quite stable. As can be seen from \figurename{s} \ref{missing-values-seconds:uccs} and \ref{missing-values-seconds:mems}, missing values at both stations are grouped in a few consecutive periods.
To deal with them when R methods need complete timeseries, we define different completion strategies, depending on the seasonality, using the R function \texttt{na.aggregate}.

\begin{figure}[t]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.uccs_files/figure-latex/missing values hour analysis-1.pdf}
		\caption{Uccle station}
		\label{missing-values-seconds:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.mems_files/figure-latex/missing values hour analysis-1.pdf}
		\caption{Membach station}
		\label{missing-values-seconds:mems}
	\end{subfigure}
	\caption{Missing values representation}
	\label{missing-values-seconds}
\end{figure}
%
\subsubsection{Seasonality assumptions}
The already cited article \cite{NatureCoronavirusSeismic} suggests the existence of a weekly seasonality. Looking at the data it seems confirmed, in addition to a daily seasonality (\figurename~\ref{fig:zoom}).

Since our main focus is on the seasonalities themselves, we aim to prevent assumptions based on this objective. In other words, we want to avoid searching for correlation with human activity while imposing human-based seasonality periods. Thus, we use an analytical method to extract seasonalities that the data itself suggests to us. Still, we recognize that the already performed transformations impose an human-based time alteration to the data itself (Section \ref{sec:cleanup-transform}).

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.uccs_files/figure-latex/whole data plot-2"}
		\caption{Uccle station}
		\label{fig:zoom:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.mems_files/figure-latex/whole data plot-2"}
		\caption{Membach station}
		\label{fig:zoom:mems}
	\end{subfigure}
	\caption{Zoom from January $6^{th}$ to February $3^{rd}$ 2019}
	\label{fig:zoom}
\end{figure}
%
The method we are going to apply is based on the Fourier Transform (FT), and generates a \textit{periodgram} highlighting the frequencies that might correspond to a seasonality component \cite[chapter 11]{brockwell1991time}.
Applying the FT for any frequency, we transform our 1-dimensional data into a 2-dimensional data mapped around a circle. Then, we can sum all the resulting vectors (that are our new data points) to obtain a vector for each frequency, whose absolute value represents the power level of the same frequency. An high power level represents a significant periodicity on that frequency in our original data.

A common operation to increase the difference between significant and non-significant periodicities is to apply a Moving Average (MA) smoothing filter with a smaller window than the interesting ones. We will use such a filter with $p=2$.

On this note we must specify that, when a significant periodicity is found, it influences smaller periodicities given by $f*\{2,3,\dots\}$ the significant frequency \cite{brockwell1991time}. Also, if any periodicity is found that is longer than the original dataset, it must be considered as simple chance, rather than serendipity.

Since we apply this method on a dataset indexed by seconds, the transformation of the frequency $f$ into hourly periodicity $p$ is obtained by $p=(1/f)/3600$.
We can analyse the resulting periodgrams for Uccle and Membach stations in \figurename{s} \ref{fig:periodgram:uccs} and \ref{fig:periodgram:mems}.
An higher power value for a frequency means that the corresponding periodicity is meaningful for describing the dataset.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.uccs_files/figure-latex/periodgram-1"}
		\caption{Uccle station}
		\label{fig:periodgram:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.mems_files/figure-latex/periodgram-1"}
		\caption{Membach station}
		\label{fig:periodgram:mems}
	\end{subfigure}
	\caption{Periodgram results}
	\label{fig:periodgram}
\end{figure}
%
Both highlight multiple 24-hour periodicities. This is given by approximation factors, since we round the divisions result to the nearest natural number. It also highlights that using a single 24-hour seasonality could leave some information on the remaining components. Still, in both stations it is the most significant periodicity.

Both stations highlight a significant 168-hour (1-week) periodicity, that also confirms our initial assumptions.

Both stations show a possibly significant 12-hour seasonality, but since it corresponds to double the frequency of the 24-hour seasonality, we can safely ignore it.

\subsubsection{Dataset division}
As best practices suggest, we are going to divide the dataset into training and testing datasets with a ratio around $85\%/15\% - 75\%/25\%$.
Since one of our objectives is to assess any significant difference from before and after the Belgium lockdown (on March $14^{th}$ the \textit{soft} lockdown, while on March $18^{th}$ the complete lockdown), we selected data from the start of our timeseries until 20-01-2020 (64 weeks $\approx 82\%$) as training, while the test follows and ends on 27-04-2020 (14 weeks $\approx 18\%$).

Having the test set span from before to after the lockdown enables us to assess significant changes in the forecast performance on the two different periods.

This initial division demostrated to be computationally heavy while fitting some models with weekly seasonality, so that it would be challenging to fit more than a couple of models.
Given this issue, we choose the training/testing sets to hold 52/18 weeks: this is the greater train-to-test ratio we deem usable ($\approx 75\%/25\%$), while maintaining computational feasibility.
The assessment of the lockdown influence will be discussed in Section~\ref{sec:methodology}.

Some R commands cannot handle timeseries with missing values, but as we have seen both stations datasets include missing values.
Also, please note that we are going to have a timeseries object indexed by weeks, so that it contains the 168 hours between each integer index.
Since we know that the longest missing values series is 90 hours long, we can apply a simple completion strategy for serving those commands a filled timeseries: we will substitute missing values with the mean of the week they are in; in R, \texttt{na.aggregate(x, floor)}.


\subsubsection{Decomposition}
In order to decompose our dataset into trend and seasonality(-ties) components, we followed these steps:
\begin{enumerate}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item Apply a smoothing filter to extract the trend component
	\item Remove the trend from the dataset and extract the seasonal figure using the local mean method
	\item Remove the trend and the seasonal components to obtain the residuals
\end{enumerate}
While doing so, we tested different smoothing filters: simple filter (p=seasonality), Spencer's 15-point filter and MA smoothing filter (f=seasonality). Since they do not produce particularly different seasonalities, we choose to keep the results obtained by applying the MA smoothing filter, for its significance in terms of seasonally adjusted data.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.uccs_files/figure-latex/extract both 24 and 168-1.pdf}
		\caption{Uccle station}
		\label{fig:decomposed:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.mems_files/figure-latex/extract both 24 and 168-1.pdf}
		\caption{Membach station}
		\label{fig:decomposed:mems}
	\end{subfigure}
	\caption{Timeseries decomposition}{\centering\small Only the second method results are shown for clarity. Week 10 includes January $1^{st}$.\\}
	\label{fig:decomposed}
\end{figure}
%
Since the periodgrams suggested the presence of two significative periodicities, we also tested the extraction of both corresponding seasonalities:
\begin{enumerate}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item Apply the MA smoothing filter (f=24) on the dataset to extract the trend component
	\item Remove the trend from the dataset and extract the 24-hour seasonal figure using the local mean method
	\item Remove the 24-hour seasonal components to obtain a deseasoned dataset
	\item Apply the MA smoothing filter (f=168) on the deseasoned dataset to extract the updated trend component
	\item Remove the trend from the already deseasoned dataset and extract the 168-hour seasonal figure using the local mean method
	\item Repeat steps 1-5 using the last deseasoned dataset until convergence (2-3 cycles)
	\item Removing the last fitted trend and the two seasonalities we obtain the final residuals
\end{enumerate}

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.uccs_files/figure-latex/extract both 24 and 168-2.pdf}
		\caption{Uccle station}
		\label{fig:direct-multi-seasonality:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.mems_files/figure-latex/extract both 24 and 168-2.pdf}
		\caption{Membach station}
		\label{fig:direct-multi-seasonality:mems}
	\end{subfigure}
	\caption{Seasonality figures}{\centering\small\textit{Direct} refers to single seasonality extraction\\\centering\textit{Multi} refers to both seasonalities extracted at the same time\\}
	\label{fig:direct-multi-seasonality}
\end{figure}
%
In \figurename~\ref{fig:direct-multi-seasonality} we see both approaches applied to both stations. Those plots show interesting properties of our seasonalities, probably given by the method (local mean) and the fact that the weekly period is an exact multiple of the daily period:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item Both ways of extraction result in the same daily seasonality extracted
	\item The sum of the daily and weekly seasonalities extracted together is the same as the weekly seasonality extracted directly
\end{itemize}
Given in particular the second property, we will only use the 168-hours seasonality while fitting our models, for both the stations.

In \figurename~\ref{fig:decomposed} we see the double decomposition results. While data from Uccle station have a clear significativity of both trend and seasonalities, data from Membach could seem not significant. Still, if we notice that the main timeseries lies between values of 0 and 100, we can find out that around half of the value will be composed by the trend, a 30\% by the joined seasonality components, and the remaining value by the residuals. We will need to apply some transformation to flatten the spikes that start from the middle of that dataset.

From tha same decomposition results, we notice that data around January $1^{st}$ show a significant drop in values. That is highlighted by both trends, where the overall minimum falls in that period, and by the residuals for Uccle data, showing a clear overestimation.

\section{Methodology}\label{sec:methodology}
Having two different datasets, we will operate on them separately and then we will compare the model interpretation.

For each station, we fit a model that generalises our training dataset in such a way that forecasting until March $1^{st}$ becomes effective, also assessing errors analytically.
Obtaining such a reliable model will be the first step to verify any significant difference forecasting over the lockdown period.

The model we fit is the ARIMA model, including the seasonality components. To identify the order of the different components described by ARIMA, we will analyse the autocorrelation and partial autocorrelation of our datasets.
As a reminder:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item non-seasonal AR process of order $p$ is described by an exponentially or sigmoidally decaying ACF, and by a cut-off of significant PACF values at lag $p$
	\item non-seasonal MA process of order $q$ is described by an exponentially or sigmoidally decaying PACF, and by a cut-off of significant ACF values at lag $q$
	\item seasonal AR process of order $P$ is described by an exponential or sigmoidal decay at seasonal lags on ACF values, and by a cut-off of significant PACF values at each seasonal lag until the $P^{th}$
	\item seasonal MA process of order $Q$ is described by an exponential or sigmoidal decay at seasonal lags on PACF values, and by a cut-off of significant ACF values at each seasonal lag until the $Q^{th}$
	\item given how significance is assessed for the ACF/PACF spikes, one over 20 spikes (5\%) could be \textit{coincidentally significant} or \textit{coincidentally \textbf{non}-significant}
\end{itemize}

The same model will be then used to forecast other 8 weeks from March $2^{nd}$, so that we can assess the forecast power over the lockdown period. Given the fact that both our series are quite stable over time (\figurename{s} \ref{fig:decomposed:uccs}, \ref{fig:mems:box-cox:data}), we can expect that the predicted variability for further forecasts will not significantly increase.

Also, we will decompose the seasonalities from data before and after the lockdown, in order to assess any significant change regarding that aspect.

Per our hypotesis, we expect the Uccle station's data to show significant changes after the lockdown, for its position near Brussels, while we expect Membach's forecasts not to show any significant change, for its position is further from busy cities.

We will not directly compare the two models, since data from the two stations have different orders of magnitude, and that difference should be firstly assessed in order to meaningfully compare the models.

\subsection{Uccle station}
\subsubsection{ARIMA}
Our training set, despite the precautions taken during the initial data cleanup and aggregation, does not satisfy the Normality assumption, even after applying the \textit{log} or \textit{Box-Cox} transformations. Thus, we will estimate the models using the original scale.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.7\linewidth]{"project.uccs_files/figure-latex/diffs/d168-acf-pacf-1"}
	\caption{ACF and PACF plots after \textit{diff} at lag 168}
	\label{fig:uccs:p-acf}
\end{figure}
%
Since our dataset shows a relevant trend component, resulting in non-constant mean, we consider the usage of the \textit{diff}\footnote{The \textit{diff} operator applies the \textit{d}-order differences. All \textit{diff} operators we apply use $d=1$.} operator to smooth the mean.
After applying the \textit{diff} operator with lag 168, in conjunction with the lag 1 \textit{diff} or not, we come up with two possible models by checking the ACF and PACF plots:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item from using both \textit{diff}s, the ACF and PACF on the long run hint to $\mathrm{ARIMA}(0,1,0)(0,1,1)_{168}$ (plots omitted for brevity)
	\item from using just the lag 168 \textit{diff}, the PACF on the first lags and the ACF/PACF combination on the long run hint to $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$ (\figurename~\ref{fig:uccs:p-acf})
\end{itemize}
We also employ the \textit{auto.arima} stepwise methodology\footnote{\label{footnote:auto.arima}The \textit{auto.arima} stepwise methodology fits different ARIMA models within a range of orders. Based on each result's information criterion (Akaike), it changes each time one of the \textit{p}, \textit{d}, \textit{q}, \textit{P}, \textit{D}, or \textit{Q} values, depending on the configured limits. These changes minimizes the information criteria of the final model.}, locking the \textit{diff} values to 0 or 1, to get some hints on the possible models.

Following both the analytical ways and the \textit{auto.arima} hints, and refining the models through further inspection of ACF/PACF plots of the residuals, we end up with three characteristic models. We selected them through the Akaike Information Criterion (AIC)\footnote{Actually, we could have used the corrected AIC (AICc) as it is usually more precise, but the values of AIC and AICc are equal to the first decimal for all our models ($\lesssim 10^{-7}\%$ difference).}, being the only three out of the fitted models with $\mathrm{AIC} < 120.000$: $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$, $\mathrm{ARIMA}(3,1,1)(0,1,1)_{168}$, and $\mathrm{ARIMA}(4,1,2)(0,1,1)_{168}$.

Also checking the Mean Absolute Percentage Error (MAPE) on the training set, they actually have the lowest three values ($\mathrm{MAPE} < 6.2\%$). Nevertheless, MAPE is not a single valid criterion for selection, since a low error predicting the training set itself does not necessarily mean a good forecast power.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.uccs_files/figure-latex/arima-35}
		\caption{ACF and PACF plots}
		\label{fig:arima-26}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{project.uccs_files/figure-latex/arima-32}
		\caption{Q-Q plot}
		\label{fig:arima-24}
	\end{subfigure}
	\caption{Plots for $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$}
	\label{fig:arima}
\end{figure}
%
The ACF and PACF of the residuals for all three models are very similar (\figurename~\ref{fig:arima-26}), even if the models themselves have great differences in meaning and in number of parameters: they highlight a clear seasonal ARMA process with periodicity 24. This shows how our previous conclusion that the 168 hours seasonality could include the 24 hours one is flawed. Unfortunately, the R function we are using does not permit to estimate multiple seasonalities\footnote{\textit{Arima} method from the \textit{forecast} R package, only specifying the seasonal and non-seasonal orders.}.

Probably related to the 24 hours periodicity that remains on the residuals, together with the non-Normality of the original data, all Q-Q plots show clear divergence on both sides of the distribution (\figurename~\ref{fig:arima-24}). The Box-Pierce test statistics accept the independence hypothesis between residuals.

Then, we assess how the forecasts behave on the test set. The MAPE for all three models more than doubles to around 17\% for $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$ and around 15\% for the other two. This is expected, since normally the model is more fitted to the training data than to unknown future data.

Also, it must be noted that a January $1^{st}$ period lies in our test dataset, that is already noticed as low-valued period (\figurename~\ref{fig:decomposed}). We can notice a clear overestimation during those couple of weeks (around week 62 on \figurename~\ref{fig:arima-forecast}).

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=1\linewidth]{project.uccs_files/figure-latex/arima-37}
		\caption{$\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$}
		\label{fig:arima-forecast:200011}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=1\linewidth]{project.uccs_files/figure-latex/arima-69}
		\caption{$\mathrm{ARIMA}(3,1,1)(0,1,1)_{168}$}
		\label{fig:arima-forecast:412011}
	\end{subfigure}
	\caption{ARIMA forecasts}{\footnotesize$\mathrm{ARIMA}(4,1,2)(0,1,1)_{168}$ behaves like $\mathrm{ARIMA}(3,1,1)(0,1,1)_{168}$ with faster increasing variablity}
	\label{fig:arima-forecast}
\end{figure}
%
We can see in \figurename~\ref{fig:arima-forecast} the forecast behaviour for our models. We can appreciate the constant span of the confidence intervals in \figurename~\ref{fig:arima-forecast:200011}, in constrast with the more complex models that have increasing intervals further away from the training set. It must be noted that the increasing intervals include negative values, that are actually impossible to obtain given the data meaning (difference between maximum and minimum measurements).

The increasing variability also make the percentage of test values falling inside the confidence intervals to be higher than normal:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$: $\approx81\%$ of data is in the $80\%$ CI and $\approx91\%$ in the $95\%$ CI
	\item $\mathrm{ARIMA}(3,1,1)(0,1,1)_{168}$: $\approx96\%$ of data is in the $80\%$ CI and $\approx98\%$ in the $95\%$ CI
	\item $\mathrm{ARIMA}(4,1,2)(0,1,1)_{168}$: $\approx98\%$ of data is in the $80\%$ CI and $\approx99\%$ in the $95\%$ CI
\end{itemize}

%Also, ARIMA parameters roots have values within the unit circle for the first two models, while for $\mathrm{ARIMA}(4,1,2)(0,1,1)_{168}$ two of the roots fall outside the unit circle, with values of $\approx1.49$ and $\approx-1.80$, making the models invalid.

Given all these factors, we are confident choosing the $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$ model, being aware of the missed 24 hours seasonality.

%\subsubsection{MSTL}
%\subsubsection{Fourier}

\subsection{Membach station}
\subsubsection{ARIMA}
Similarly to data from Uccle, data from Membach doesn't respect the Normality assumption. We tested if the Q-Q plot showed significantly better approximation of the Normal distribution after applying the \textit{log} or \textit{Box-Cox} transformations, and the latter makes the data approach Normality with $\lambda\approx-1$ (\figurename~\ref{fig:mems:box-cox}). This solves the high peaks problem we already noticed while decomposing to highlight the trend and seasonalities.

Since the $\lambda$ value approximately simplifies the Box-Cox transformation formula from $(y^\lambda-1)/\lambda$ into $(y-1)/y$, and since we only have positive values, all transformed values fall into the higher $0-1$ spectrum, in particular $0.95-1$ (\figurename~\ref{fig:mems:box-cox:data}). This will probably cause some approximation errors that could rig the final results.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=1\linewidth]{"project.mems_files/figure-latex/training data transformation plots-7"}
		\caption{Transformed training set}
		\label{fig:mems:box-cox:data}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=1\linewidth]{"project.mems_files/figure-latex/training data transformation plots-9"}
		\caption{Normal vs. transformed data distribution}
		\label{fig:mems:box-cox:qq}
	\end{subfigure}
	\caption{Results of Box-Cox transformation}
	\label{fig:mems:box-cox}
\end{figure}
%
The transformed data still shows no constant mean nor variance, even if they show less variability around the peaks with respect to the original data, confirmed by the Q-Q plot (\figurename~\ref{fig:mems:box-cox:qq}). Thus, we consider the usage of the \textit{diff} operator, at lag 1 and 168. The analysis of ACF and PACF plots hints to two models: $\mathrm{ARIMA}(0,1,0)(0,1,1)_{168}$ and $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$. All ACF and PACF plots for Membach model estimation are not shown for brevity.

In this case, the employment of \textit{auto.arima} stepwise methodology$^{\ref{footnote:auto.arima}}$ highlights the first model improved with one non-seasonal MA parameter more. This is confirmed by the ACF/PACF plots of its residuals. Their analysis for the second model do not lead to any improvement.
The ACF and PACF plots of both models are similar to the ones from the chosen model for the Uccle station (\figurename~\ref{fig:arima-24}), and we can notice the same 24 hours remaining periodicity.

The Box-Pierce test statistics for both models accept the independence hypothesis between residuals. The two models have similar AIC values, and similar MAPE on the training set ($\approx6\%$). Thus, we can assess the forecasts for both models in \figurename~\ref{fig:mems:arima}.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=1\linewidth]{"project.mems_files/figure-latex/arima-21"}
		\caption{$\mathrm{ARIMA}(0,1,1)(0,1,1)_{168}$}
		\label{fig:mems:arima:011011}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=1\linewidth]{"project.mems_files/figure-latex/arima-13"}
		\caption{$\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$}
		\label{fig:mems:arima:200011}
	\end{subfigure}
	\caption{ARIMA forecasts}
	\label{fig:mems:arima}
\end{figure}
%
The first model catches a rapidly-increasing trend, while the latter predicts more stable values moving the most part of the increasing trend into slightly increasing variability. Visually, both could be acceptable, even if the latter seems to better behave on \textit{normal} days losing precision on high-valued ones, while the first tries to better accomodate the high-valued days losing precision on the \textit{normal} ones.

Also, it is important to notice that while the first model includes in the confidence intervals negative values, non existent in our domain, the latter directs all the variability to higher values, while maintaining a consistent lower bound. This appears to be a good feature to correctly model our data.

Analytically, we have quite a difference between the two models in terms of adherence of the confidence intervals:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item $\mathrm{ARIMA}(0,1,1)(0,1,1)_{168}$: $\approx98\%$ of data is in the $80\%$ CI and $\approx99\%$ in the $95\%$ CI
	\item $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$: $\approx73\%$ of data is in the $80\%$ CI and $\approx89\%$ in the $95\%$ CI
\end{itemize}
As we see, the first model over-generalises the variability, while the latter seems more precise, even if losing some information. Also considering the MAPE on the test set, the latter model fits the data better with a $16\%$ error, instead of the $18\%$ for the first model.
%
%Parameters roots for both models have values within the unit circle.

Thus, we can select the $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$ model on the Box-Cox($\lambda\approx-1$) transformed data, still being aware of the missed 24 hours seasonality, similarly to the Uccle station.

Since checking the ACF and PACF plots for the original data could lead to a model with the same order, we fitted it for comparison. The MAPEs are higher but comparable, $7\%$ on the test set and $19\%$ on the train set, the Box-Pierce test refutes the independence between residuals, and $88\%/94\%$ of test data fits inside the $80\%$ and $95\%$ CI. Finally, it shows a variability that tends to admit lower values than the training data shows, including negative ones, while having a lower acceptance for higher values. We omit plots related to this model, since such results confirm the choice of the model on the transformed data.

It is important to highlight that no model we assessed found any pattern for predicting the high-valued spikes.
%\subsubsection{MSTL}

\subsection{Lockdown influence}
Even if the two models have the same order, they cannot be directly compared using the Information Criteria, for the Membach model works after a transformation pass, and they are fitted from different data.

Still, we can assess how the lockdown period is forecasted for both stations, and then compare their efficency. Also, we can extract the seasonalities, using the same MA filters we already used, separately from the periods before and during the lockdown, and compare also these results.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.uccs_files/figure-latex/seasonalities/before-after-compare-1"}
		\caption{Uccle station}
		\label{fig:season.lockdown:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.mems_files/figure-latex/seasonalities/before-after-compare-1"}
		\caption{Membach station}
		\label{fig:season.lockdown:mems}
	\end{subfigure}
	\caption{Seasonalities computed before and after the lockdown}{\centering\small Seasonalities extracted together with MA filters, confidence bands $\pm\mathrm{sd}$\\}
	\label{fig:season.lockdown}
\end{figure}
%
\subsubsection{Seasonalities divergence}
We highlight the confidence bands for an estimated error of 67\%. This interval estimation lies on the assumption that values for each single hour in the hourly/weekly seasonality is Normally distributed, a slightly different assumption that is not necessarily refuted by the already seen Q-Q plots.

As we can see in \figurename~\ref{fig:season.lockdown}, the seasonalities' shapes seem different, in particular comparing the weekly seasonality. The seasonalities from before and after the lockdown are still included in the highlighted confidence bands, apart from a few hours. This is true for both stations.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.mems_files/figure-latex/seasonalities/seasonplot-mean-sd-1"}
		\caption{Before lockdown}
		\label{fig:season.lockdown:msb:mems}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.mems_files/figure-latex/seasonalities/seasonplot-mean-sd-2"}
		\caption{After lockdown}
		\label{fig:season.lockdown:msa:mems}
	\end{subfigure}
	\caption{Seasonal plots for Membach station}
	\label{fig:season.lockdown:ms:mems}
\end{figure}
%
The differences in shape are more highlighted checking the seasonal plots \cite{hyndman2018forecasting} in \figurename~\ref{fig:season.lockdown:ms:mems}. It is possible to notice a couple of changes: the Monday to Friday shapes are heavily flattened almost on the weekend values, and all the values are flattened towards 0. Both flattenings can be noticed also checking the Uccle station, not showed for brevity.

\subsubsection{Forecast divergence}
Since both our models forecast stable values with almost stable variance (\figurename{s}~\ref{fig:arima-forecast:200011}, \ref{fig:mems:arima:200011}), we can forecast another 8 weeks ahead, until April $26^{th}$, and then compare the efficency of the results with the original test set. These 8 weeks comprise 6 weeks after March $14^{th}$, first day of lockdown in Belgium.

As we can see in \figurename~\ref{fig:forecast.lockdown}, at both stations the models overestimate starting after the lockdown. Uccle data in particular show a greater overestimation, that can also be noticed by looking at the difference between the forecasted and measured values.

\begin{figure}[h]
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.uccs_files/figure-latex/final-model-5"}
		\caption{Uccle station}
		\label{fig:forecast.lockdown:uccs}
	\end{subfigure}
	\begin{subfigure}{.5\linewidth}
		\includegraphics[width=\linewidth]{"project.mems_files/figure-latex/final-model-5"}
		\caption{Membach station}
		\label{fig:forecast.lockdown:mems}
	\end{subfigure}
	\caption{Forecasts from March $2^{nd}$.}{\centering\small In red the actual values, in black the forecasts. March $14^{th}$ at midnight is highlighted.\\}
	\label{fig:forecast.lockdown}
\end{figure}
%
We assess analytical values obtained from the new 8 weeks forecast:
\begin{itemize}[topsep=0.5em,itemsep=0em,partopsep=0.5em]
	\item the MAPE for Uccle triples with respect to the original test set to $\approx46\%$, and only 39\% and 63\% of the test data falls inside the confidence intevals (80\% and 95\%).
	\item regarding the Membach station forecasts, the MAPE is reduced to $14\%$, while 82\% and 96\% of data falls inside the confidence intervals.
\end{itemize}

Thus, we notice a visible degradation in performance for Uccle station data, and a slight improvement for Membach station's. The slight improvement is most certainly driven by the single peak, with lower values than previous peaks, and by the increased variability for data further from the end of the training set.


%MEMS
%Test set inside the CI
%80% CI 	  74.18 %
%95% CI 	  89.58 %
%Errors:  forecast with ARIMA(2,0,0)(0,1,1)[168] 
%MSE= 117.9258   MAE= 7.709243   MAPE= 16.60103   RMSE= 10.85937   R2= 0.4777765 


%UCCS
%Test set inside the CI
%80% CI 	  57.51 %
%95% CI 	  78.42 %
%Errors:  forecast with ARIMA(2,0,0)(0,1,1)[168] 
%MSE= 670897.8   MAE= 664.5903   MAPE= 33.94793   RMSE= 819.0835   R2= 0.5188538 


%Uccle: remodel on 52+18 (until 1st March 2020), forecast 2 months (8 weeks) => diff in MAPE
%Membach: remodel on 52+18 (until 1st March 2020), forecast 2 months (8 weeks) => diff in MAPE

\section{Conclusions}
The models we choose for both stations show interesting predictive efficiency, 

TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 

Interestingly, the analysis of ACF and PACF plots for Membach hints to similar models to the untransformed Uccle data:
$\mathrm{ARIMA}(0,1,0)(0,1,1)_{168}$ and $\mathrm{ARIMA}(2,0,0)(0,1,1)_{168}$.

TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO TODO 

\subsubsection{Lockdown influence}
\subsection{Limitations and future works}
forecast residuals of model with 24h seasonality, and 

seasonalities divergence -> check normality assumption for each hour to validate the gaussian-empirical confidence bands

The main problems of this projects that we identified are multiple seasonalities and the Normality assumption.

% comment by prof on some arima plots
% If you're trying to compare what happens before and after lock-down, you should probably assess the forecast for the period without lock-down and then compare with how it performs for forecasting lock-down values. A large difference (forecast performing worse during lock-down) would indicate that the model doesn't work during lock-down because something has changed in the behaviour of the data, thus confirming your hypothesis

The tested models do not effectively include both daily and weekly seasonalities, even if initially we thought so. Also they cannot include the apparent yearly seasonality, mainly because of the short timeseries.
To overcome this limitation, we could try different models, also ARIMA-based, that can estimate multiple seasonalities. The first that we tested models seasonalites through Fourier terms. The main problem that lead us not to go further with that approach is that our seasonalities cannot be expressed by a combination of few sine and cosine curves. This requires a high number of curves to estimate, that requires an heavy computational power.

To compensate those difficulties, in the literature it is suggested to test TBATS models, which combines Fourier terms, Box-Cox transformation, and exponential smoothing. This seems a suitable approach given the analyses we performed until now, that can possibly address the non-Normal distribution.

The final suggestion for future work would be to better analyse the differences between stations, in such a way to better correlate different influences of the lockdown on the areas near each station.




\newpage
\bibliography{project}

\end{document}
